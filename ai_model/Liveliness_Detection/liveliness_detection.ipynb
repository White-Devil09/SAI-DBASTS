{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install resampy\n",
    "# %pip install tqdm\n",
    "# %pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"./Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(audio_file_path, Tag):\n",
    "    ad, sr = librosa.load(audio_file_path)\n",
    "\n",
    "    spec = np.abs(librosa.stft(ad))\n",
    "    spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
    "\n",
    "    mel_spect = librosa.feature.melspectrogram(y=ad, sr=sr)\n",
    "    mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "\n",
    "    chroma = librosa.feature.chroma_cqt(y=ad, sr=sr, bins_per_octave=36)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=ad, sr=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(5, 1, figsize=(10, 15), constrained_layout=True)\n",
    "    fig.suptitle(f'Visualization of {Tag} audio', fontsize=16)\n",
    "\n",
    "    ax[0].plot(ad)\n",
    "    ax[0].set_title(f'{Tag} audio Waveform')\n",
    "\n",
    "    img = librosa.display.specshow(spec, sr=sr, x_axis='time', y_axis='log', ax=ax[1])\n",
    "    fig.colorbar(img, ax=ax[1], format=\"%+2.0f dB\")\n",
    "    ax[1].set_title(f'{Tag} audio spectrogram')\n",
    "\n",
    "    img = librosa.display.specshow(mel_spect, sr=sr, x_axis='time', y_axis='mel', ax=ax[2])\n",
    "    fig.colorbar(img, ax=ax[2], format=\"%+2.0f dB\")\n",
    "    ax[2].set_title(f'{Tag} audio Mel Spectrogram')\n",
    "\n",
    "    img = librosa.display.specshow(chroma, sr=sr, x_axis='time', y_axis='chroma', ax=ax[3])\n",
    "    fig.colorbar(img, ax=ax[3], format=\"%+2.0f dB\")\n",
    "    ax[3].set_title(f'{Tag} audio Chroma')\n",
    "\n",
    "    img = librosa.display.specshow(mfcc, sr=sr, x_axis='time', ax=ax[4])\n",
    "    fig.colorbar(img, ax=ax[4], format=\"%+2.0f dB\")\n",
    "    ax[4].set_title(f'{Tag} audio MFCC')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_audio_paths = os.path.join(audio_file_path, 'DEMO')\n",
    "\n",
    "for item in os.listdir(demo_audio_paths):\n",
    "    if item.find(\"original\") != -1:\n",
    "        real_audio_path = os.path.join(demo_audio_paths, item)\n",
    "    else:\n",
    "        fake_audio_path = os.path.join(demo_audio_paths, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(real_audio_path, \"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(fake_audio_path, \"Fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "\n",
    "folders = ['FAKE', 'REAL']\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(audio_file_path, folder))\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        file_path = os.path.join(audio_file_path, folder, file)\n",
    "        audio, sr = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "        mfcc_features_scaled = np.mean(mfcc_features.T, axis=0)\n",
    "\n",
    "        data.append(mfcc_features_scaled)\n",
    "        labels.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame({'features': data, 'labels': labels})\n",
    "print(feature_df.head())\n",
    "print(feature_df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(labels):\n",
    "    le = LabelEncoder().fit(labels)\n",
    "    print(labels.name, le.classes_)\n",
    "    return le.transform(labels)\n",
    "\n",
    "feature_df['labels'] = label_encoder(feature_df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(feature_df.features.tolist())\n",
    "y = np.array(feature_df.labels.tolist())\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled = to_categorical(y_resampled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "num_labels = len(feature_df['labels'].unique())\n",
    "input_shape = feature_df['features'][0].shape\n",
    "\n",
    "print(f\"Input shape : {input_shape}\")\n",
    "print(f\"Number of labels : {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_shape=input_shape),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(256),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(128),\n",
    "    Activation(\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_labels),\n",
    "    Activation(\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=2, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss : {test_loss}\")\n",
    "print(f\"Test accuracy : {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(audio_file_path, model):\n",
    "    audio, sr = librosa.load(audio_file_path, res_type='kaiser_fast')\n",
    "    mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "    mfcc_features_scaled = np.mean(mfcc_features.T, axis=0)\n",
    "    mfcc_features_scaled = np.expand_dims(mfcc_features_scaled, axis=0)\n",
    "    prediction = model.predict(mfcc_features_scaled)\n",
    "\n",
    "    print(f\"Prediction : {folders[np.argmax(prediction[0])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path = '../Speaker_Identification/combined_files/Benjamin_Netanyau_combined.wav'\n",
    "predict(real_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currTime = pd.Timestamp.now().strftime(\"%Y%m%d%H%M\")\n",
    "model.save(f'weights/anti-spoof-{currTime}.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
